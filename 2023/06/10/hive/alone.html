<hr>
<h2 id="title-Spark-stand-alone"><a href="#title-Spark-stand-alone" class="headerlink" title="title: Spark(stand-alone)"></a>title: Spark(stand-alone)</h2><p>Spark(stand-alone)</p>
<!--more-->
<h1 id="1-进入spark配置文件目录中：cd-spark-x2F-conf"><a href="#1-进入spark配置文件目录中：cd-spark-x2F-conf" class="headerlink" title="1.进入spark配置文件目录中：cd  spark&#x2F;conf"></a>1.进入spark配置文件目录中：cd  spark&#x2F;conf</h1><pre><code class="bash">配置workers文件:
（1）改名, 去除.template后缀：mv workers.template workers
（2）编辑worker文件：vim workers
（3）删除localhost, 添加node1，node2，node3到workers文件内
</code></pre>
<h1 id="2-配置spark-env-sh文件"><a href="#2-配置spark-env-sh文件" class="headerlink" title="2.配置spark-env.sh文件"></a>2.配置spark-env.sh文件</h1><p>（1）改名</p>
<pre><code class="bash">mv spark-env.sh.template spark-env.sh
</code></pre>
<p><img src="/./alone/a1.png" alt="alone"></p>
<p>（2）编辑spark-env.sh, 在底部追加如下内容<br> <img src="/./alone/a2.png" alt="alone"></p>
<h1 id="3-在HDFS上创建程序运行历史记录存放的文件夹"><a href="#3-在HDFS上创建程序运行历史记录存放的文件夹" class="headerlink" title="3.在HDFS上创建程序运行历史记录存放的文件夹:"></a>3.在HDFS上创建程序运行历史记录存放的文件夹:</h1><pre><code class="bash">hadoop fs -mkdir /sparklog
hadoop fs -chmod 777 /sparklog
</code></pre>
<p> <img src="/./alone/a3.png" alt="alone"></p>
<h1 id="4-配置spark-defaults-conf文件"><a href="#4-配置spark-defaults-conf文件" class="headerlink" title="4.配置spark-defaults.conf文件"></a>4.配置spark-defaults.conf文件</h1><p>#1. 改名</p>
<pre><code class="bash">mv spark-defaults.conf.template spark-defaults.conf
</code></pre>
<p> <img src="/./alone/a4.png" alt="alone"></p>
<p>#2. 修改内容, 追加如下内容<br> 开启spark的日期记录功能</p>
<pre><code class="bash">spark.eventLog.enabled 	true
</code></pre>
<p> 设置spark日志记录的路径</p>
<pre><code class="bash">spark.eventLog.dir	 hdfs://node1:8020/sparklog/ 
</code></pre>
<p> 设置spark日志是否启动压缩</p>
<pre><code class="bash">spark.eventLog.compress 	true
</code></pre>
<p> <img src="/./alone/a5.png" alt="alone"></p>
<p>将Spark安装文件夹，分发到node1,node2上</p>
<pre><code class="bash">scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/
scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/
</code></pre>
<p>分别在node2，node3设置软连接：</p>
<pre><code class="bash">ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark
</code></pre>
<p>启动历史服务器</p>
<pre><code class="bash">sbin/start-history-server.sh
</code></pre>
<p>启动Spark的Master和worker进程<br>#启动全部master和worker</p>
<pre><code class="bash">sbin/start-all.sh
</code></pre>
<p>查看Master的WEB UI</p>
<h1 id="Spark-StandAlone环境部署"><a href="#Spark-StandAlone环境部署" class="headerlink" title="Spark StandAlone环境部署"></a>Spark StandAlone环境部署</h1><pre><code class="bash">1.进入spark配置文件目录中：cd  spark/conf
配置workers文件:
（1）改名, 去除.template后缀：mv workers.template workers
（2）编辑worker文件：vim workers
（3）删除localhost, 添加node1，node2，node3到workers文件内
</code></pre>
<p>2.配置spark-env.sh文件<br>（1）改名</p>
<pre><code class="bash">mv spark-env.sh.template spark-env.sh
</code></pre>
<p> <img src="/./alone/a6.png" alt="alone"></p>
<p>（3）编辑spark-env.sh, 在底部追加如下内容<br> <img src="/./alone/a7.png" alt="alone"></p>
<p>3.在HDFS上创建程序运行历史记录存放的文件夹:</p>
<pre><code class="bash">hadoop fs -mkdir /sparklog
hadoop fs -chmod 777 /sparklog
</code></pre>
<p> <img src="/./alone/a8.png" alt="alone"></p>
<p>4.配置spark-defaults.conf文件<br> #1. 改名</p>
<pre><code class="bash">mv spark-defaults.conf.template spark-defaults.conf
</code></pre>
<p> <img src="/./alone/a9.png" alt="alone"></p>
<pre><code class="bash">#2. 修改内容, 追加如下内容
#开启spark的日期记录功能
spark.eventLog.enabled 	true
设置spark日志记录的路径
spark.eventLog.dir	 hdfs://node1:8020/sparklog/ 
设置spark日志是否启动压缩
spark.eventLog.compress 	true
</code></pre>
<p> <img src="/./alone/a10.png" alt="alone"></p>
<p>将Spark安装文件夹，分发到node1,node2上</p>
<pre><code class="bash">scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/
scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/
分别在node2，node3设置软连接：
ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark
启动历史服务器
sbin/start-history-server.sh
启动Spark的Master和worker进程
启动全部master和worker
sbin/start-all.sh
查看Master的WEB UI
</code></pre>
<p> <img src="/./alone/a11.png" alt="alone"></p>
<p>连接到StandAlone集群</p>
<pre><code class="bash">bin/pyspark --master spark://node1:7077
</code></pre>
<p> <img src="/./alone/a12.png" alt="alone"></p>
<p>查看历史服务器WEB UI<br> <img src="/./alone/a13.png" alt="alone"></p>
